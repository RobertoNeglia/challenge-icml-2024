{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "1. Dataset Loading\n",
    "Implements the pipeline to load a dataset from the src domain. Since the challenge repository doesn’t allow storing large files, loaders must download datasets from external sources into the datasets/ folder.\n",
    "This pipeline is provided for several graph-based datasets. For any other src domain, participants are allowed to transform graph datasets into the corresponding domain through our provided lifting mappings –or just dropping their connectivity to get point-clouds.\n",
    "(Bonus) Designing a loader for a new dataset (ones that are not already provided in the tutorials) will be positively taken into consideration in the final evaluation.\n",
    "\n",
    "2. Pre-processing the Dataset\n",
    "Applies the lifting transform to the dataset.\n",
    "Needs to be done through the PreProcessor, which we provide in\n",
    "modules/io/preprocess/preprocessor.py.\n",
    "\n",
    "3. Running a Model over the Lifted Dataset \n",
    "Creates a Neural Network model that operates over the dst domain, leveraging TopoModelX for higher order topologies or torch_geometric for graphs.\n",
    "Runs the model on the lifted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pyflagsercount as pfc\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from modules.transforms.liftings.graph2combinatorial.sp_lifting import (\n",
    "    DirectedFlagComplex as dfc,\n",
    ")\n",
    "\n",
    "# from datasets.data_loading import get_dataset, get_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# With this cell any imported module is reloaded before each cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.data.load.loaders import GraphLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAMELEON = \"chameleon\"\n",
    "CORNELL = \"Cornell\"\n",
    "WISCONSIN = \"Wisconsin\"\n",
    "TEXAS = \"Texas\"\n",
    "ROMAN_EMPIRE = \"directed-roman-empire\"\n",
    "SQUIRREL = \"squirrel\"\n",
    "OGBN_ARXIV = \"ogbn-arxiv\"\n",
    "SNAP_PATENTS = \"snap-patents\"\n",
    "CORA_ML = \"cora_ml\"\n",
    "CITESEER_FULL = \"citeseer_full\"\n",
    "ARXIV_YEAR = \"arxiv-year\"\n",
    "SYN_DIR = \"syn-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration for cocitation_cora:\n",
      "\n",
      "{'data_domain': 'graph',\n",
      " 'data_type': 'cocitation',\n",
      " 'data_name': 'Cora',\n",
      " 'data_dir': 'datasets/graph/cocitation',\n",
      " 'num_features': 1433,\n",
      " 'num_classes': 7,\n",
      " 'task': 'classification',\n",
      " 'loss_type': 'cross_entropy',\n",
      " 'monitor_metric': 'accuracy',\n",
      " 'task_level': 'node'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cocitation_cora\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = GraphLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset only contains 1 sample:\n",
      " - Graph with 2708 vertices and 10556 edges.\n",
      " - Features dimensions: [1433, 0]\n",
      " - There are 0 isolated nodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = loader.load()\n",
    "describe_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10556])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform configuration for graph2combinatorial/sp_lifting:\n",
      "\n",
      "{'transform_type': 'lifting',\n",
      " 'transform_name': 'Graph2CombinatorialLifting',\n",
      " 'd1': 2,\n",
      " 'd2': 2,\n",
      " 'q': 1,\n",
      " 'i': 0,\n",
      " 'j': 2,\n",
      " 'complex_dim': 2,\n",
      " 'offset': 'torch.tensor([[0], [0]])',\n",
      " 'chunk_size': 1024,\n",
      " 'save_path': 'None',\n",
      " 'threshold': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define transformation type and id\n",
    "transform_type = \"liftings\"\n",
    "# If the transform is a topological lifting, it should include both the type of the lifting and the identifier\n",
    "transform_id = \"graph2combinatorial/sp_lifting\"\n",
    "\n",
    "# Read yaml file\n",
    "transform_config = {\n",
    "    \"lifting\": load_transform_config(transform_type, transform_id)\n",
    "    # other transforms (e.g. data manipulations, feature liftings) can be added here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifted_dataset = PreProcessor(dataset, transform_config, loader.data_dir)\n",
    "describe_data(lifted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "\n",
    "# def create_csv_datasets(dataset_name, dataset_dir=\"../dataset/\"):\n",
    "#     dataset, evaluator = get_dataset(dataset_name, dataset_dir)\n",
    "#     source = dataset.edge_index[0].tolist()  # source\n",
    "#     target = dataset.edge_index[1].tolist()  # target\n",
    "\n",
    "#     csv_file_name = \"./dataset/vis/original/\" + dataset_name + \".csv\"\n",
    "\n",
    "#     with open(csv_file_name, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "\n",
    "#         # Write the list content as rows\n",
    "#         for a, b in zip(source, target):\n",
    "#             writer.writerow([a, b])\n",
    "\n",
    "#     print(f'CSV file \"{csv_file_name}\" created successfully.')\n",
    "\n",
    "\n",
    "# def create_csv_condensations(dataset_name):\n",
    "\n",
    "#     dataset_digraph = create_digraph_from_dataset(dataset_name)\n",
    "#     condensation_digraph = nx.condensation(dataset_digraph)\n",
    "\n",
    "#     condensation_digraph_edges = list(condensation_digraph.edges)\n",
    "\n",
    "#     if dataset_name == \"cora_ml\":\n",
    "#         dataset_name = \"cora-ml\"\n",
    "\n",
    "#     if dataset_name == \"citeseer_full\":\n",
    "#         dataset_name = \"citeseer-full\"\n",
    "\n",
    "#     csv_file_name = \"./dataset/vis/condensations/\" + dataset_name + \"-condensation.csv\"\n",
    "\n",
    "#     with open(csv_file_name, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "\n",
    "#         # Write the list content as rows\n",
    "#         for e in condensation_digraph_edges:\n",
    "#             writer.writerow([e[0], e[1]])\n",
    "\n",
    "#     print(f'CSV file \"{csv_file_name}\" created successfully.')\n",
    "\n",
    "\n",
    "# def create_csv_condensations_from_dataset():\n",
    "\n",
    "#     dataset_list = [\n",
    "#         CHAMELEON,\n",
    "#         ROMAN_EMPIRE,\n",
    "#         SQUIRREL,\n",
    "#         OGBN_ARXIV,\n",
    "#         CORA_ML,\n",
    "#         CITESEER_FULL,\n",
    "#         ARXIV_YEAR,\n",
    "#     ]\n",
    "\n",
    "#     for dataset in dataset_list:\n",
    "#         create_csv_datasets(dataset)\n",
    "#         create_csv_condensations(dataset)\n",
    "\n",
    "\n",
    "# def flagser_count(dataset_name, complex_dim=2, num_threads=4):\n",
    "#     dataset_digraph = create_digraph_from_dataset(dataset_name)\n",
    "\n",
    "#     sparse_adjacency_matrix = nx.to_scipy_sparse_array(dataset_digraph, format=\"csr\")\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     X = pfc.flagser_count(\n",
    "#         sparse_adjacency_matrix,\n",
    "#         threads=num_threads,\n",
    "#         return_simplices=True,\n",
    "#         max_dim=complex_dim,\n",
    "#     )\n",
    "#     end_time = time.time()\n",
    "#     print(\"Time elapsed: \", end_time - start_time)\n",
    "#     return X\n",
    "\n",
    "\n",
    "# def create_digraph_from_dataset(dataset_name, dataset_dir=\"../dataset/\"):\n",
    "#     dataset, evaluator = get_dataset(dataset_name, dataset_dir)\n",
    "#     dataset_digraph = nx.DiGraph()\n",
    "#     dataset_digraph.add_edges_from(\n",
    "#         list(zip(dataset.edge_index[0].tolist(), dataset.edge_index[1].tolist()))\n",
    "#     )\n",
    "#     print(\"Number of nodes: \", dataset_digraph.number_of_nodes(), \" Number of edges: \", dataset_digraph.number_of_edges())\n",
    "#     return dataset_digraph\n",
    "\n",
    "\n",
    "# def create_flag_complex_from_dataset(dataset_name, dataset_dir, complex_dim=2):\n",
    "#     dataset_digraph = create_digraph_from_dataset(dataset_name, dataset_dir)\n",
    "#     flag_complex = dfc.DirectedFlagComplex(dataset_digraph, complex_dim)\n",
    "#     return flag_complex\n",
    "\n",
    "\n",
    "# def create_condensed_digraph_from_dataset(dataset_name):\n",
    "#     dataset_digraph = create_digraph_from_dataset(dataset_name)\n",
    "#     condensation_digraph = nx.condensation(dataset_digraph)\n",
    "#     return condensation_digraph\n",
    "\n",
    "\n",
    "# def dataset_stats(dataset_name, complex_dim=2):\n",
    "#     G = create_digraph_from_dataset(dataset_name)\n",
    "#     FlG = dfc.DirectedFlagComplex(G, complex_dim)\n",
    "#     for d in range(complex_dim + 1):\n",
    "#         print(dataset_name + \" number of \" + str(d) + \"-simplices\", len(FlG.complex[d]))\n",
    "#     return FlG\n",
    "\n",
    "\n",
    "# def qij(\n",
    "#     dataset_name, dataset_dir, q, i, j, complex_dim=2, chunk_size=1024, save_path=None\n",
    "# ):\n",
    "#     FlG = create_flag_complex_from_dataset(dataset_name, dataset_dir, complex_dim)\n",
    "#     return FlG.qij(q, i, j, chunk_size, save_path)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATASET_DIR = \"../../dataset/\"\n",
    "#     qij(\n",
    "#         WISCONSIN,\n",
    "#         DATASET_DIR,\n",
    "#         1,\n",
    "#         0,\n",
    "#         2,\n",
    "#         complex_dim=2,\n",
    "#         chunk_size=100,\n",
    "#         # save_path=\"../../dataset/cornell/102.pt\",\n",
    "#         save_path=None\n",
    "#     )\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
