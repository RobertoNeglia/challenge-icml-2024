{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-to-Combinatorial GraphCurve Lifting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook shows how to import a dataset, with the desired lifting, and how to run a neural network using the loaded data.\n",
    "\n",
    "The notebook is divided into sections:\n",
    "\n",
    "- [Loading the dataset](#loading-the-dataset) loads the config files for the data and the desired tranformation, createsa a dataset object and visualizes it.\n",
    "- [Loading and applying the lifting](#loading-and-applying-the-lifting) defines a simple neural network to test that the lifting creates the expected incidence matrices.\n",
    "- [Create and run a simplicial nn model](#create-and-run-a-simplicial-nn-model) simply runs a forward pass of the model to check that everything is working as expected.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Note that for simplicity the notebook is setup to use a simple graph. However, there is a set of available datasets that you can play with.\n",
    "\n",
    "To switch to one of the available datasets, simply change the *dataset_name* variable in [Dataset config](#dataset-config) to one of the following names:\n",
    "\n",
    "* cocitation_cora\n",
    "* cocitation_citeseer\n",
    "* cocitation_pubmed\n",
    "* MUTAG\n",
    "* NCI1\n",
    "* NCI109\n",
    "* PROTEINS_TU\n",
    "* AQSOL\n",
    "* ZINC\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this cell any imported module is reloaded before each cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.data.load.loaders import GraphLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just need to spicify the name of the available dataset that we want to load. First, the dataset config is read from the corresponding yaml file (located at `/configs/datasets/` directory), and then the data is loaded via the implemented `Loaders`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"manual_dataset\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = GraphLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access to the data through the `load()`method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader.load()\n",
    "describe_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Applying the Lifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will instantiate the lifting we want to apply to the data. For this example the clique lifting was chosen. For a clique of n nodes the algorithm for $m=3,...,max(n, complex\\_dim)$ will create simplicials for every possible combinations containing m nodes of the clique. $complex\\_dim$ is a parameter of the lifting. This is a deterministic lifting, based on connectivity, that does not modify the initial connectivity of the graph. The problem of extracting all the cliques in a graph is NP-hard, on in some formulaitons NP-complete (clique decision problem). The computational complexity of this algorithm is $O(n^k k^2)$[[1]](https://www.sciencedirect.com/science/article/pii/S0019995885800413), where $n$ is the number of nodes in the graph and $k$ is the highest clique dimension considered.\n",
    "\n",
    "***\n",
    "[[1]](https://www.sciencedirect.com/science/article/pii/S0019995885800413) Cook, S. A. (1985). A taxonomy of problems with fast parallel algorithms. Information and control, 64(1-3), 2-22.\n",
    "***\n",
    "\n",
    "For simplicial complexes creating a lifting involves creating a `SimplicialComplex` object from topomodelx and adding simplices to it using the method `add_simplices_from`. The `SimplicialComplex` class then takes care of creating all the needed matrices.\n",
    "\n",
    "Similarly to before, we can specify the transformation we want to apply through its type and id --the correxponding config files located at `/configs/transforms`. \n",
    "\n",
    "Note that the *tranform_config* dictionary generated below can contain a sequence of tranforms if it is needed.\n",
    "\n",
    "This can also be used to explore liftings from one topological domain to another, for example using two liftings it is possible to achieve a sequence such as: graph -> simplicial complex -> hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation type and id\n",
    "transform_type = \"liftings\"\n",
    "# If the transform is a topological lifting, it should include both the type of the lifting and the identifier\n",
    "transform_id = \"graph2combinatorial/curve_lifting\"\n",
    "\n",
    "# Read yaml file\n",
    "transform_config = {\n",
    "    \"lifting\": load_transform_config(transform_type, transform_id)\n",
    "    # other transforms (e.g. data manipulations, feature liftings) can be added here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than apply the transform via our `PreProcesor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifted_dataset = PreProcessor(dataset, transform_config, loader.data_dir)\n",
    "describe_data(lifted_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run a Simplicial NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a simple model is created to test that the used lifting works as intended. In this case the model uses the `up_laplacian_1` and the `down_laplacian_1` so the lifting should make sure to add them to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models.simplicial.san import SANModel\n",
    "\n",
    "model_type = \"d\"\n",
    "model_id = \"san\"\n",
    "model_config = load_model_config(model_type, model_id)\n",
    "\n",
    "model = SANModel(model_config, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(lifted_dataset.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the cell above should execute without errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
