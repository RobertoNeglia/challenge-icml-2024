{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointcloud-to-Hypergraph PointNet Lifting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this cell any imported module is reloaded before each cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import rootutils\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from modules.data.load.loaders import SimplicialLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset - Wall Shear Stress on the Artery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is described in detail in [this paper](https://arxiv.org/abs/2212.05023). Wall shear stress is a useful medical biomarker that has been linked to coronary artery disease. This quantity can be roughly estimated from the artery shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"wall_shear_stress\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = SimplicialLoader(dataset_config)\n",
    "\n",
    "dataset = loader.load()\n",
    "\n",
    "print(\"\\nDataset:\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of 2000 triangular meshes which are concatenated and represented as simplicial complex. The `slices_pos` and `slices_face` can be used to extract individual meshes. The node attribute is the geodesic distance to the artery inlet (to give the mesh a direction) and the face attribute is the surface normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "# Extract individual sample\n",
    "slice_pos = slice(dataset.slices_pos[sample_idx], dataset.slices_pos[sample_idx + 1])\n",
    "slice_face = slice(dataset.slices_face[sample_idx], dataset.slices_face[sample_idx + 1])\n",
    "\n",
    "data = Data(\n",
    "    x=dataset.x[slice_pos],\n",
    "    y=dataset.y[slice_pos],\n",
    "    pos=dataset.pos[slice_pos],\n",
    "    face=dataset.face[:, slice_face],\n",
    "    # x_2=dataset.x_2[slice_face],\n",
    "    # incidence_2=dataset.incidence_2[slice_pos, slice_face]  # not supported by PyTorch,\n",
    "    num_features=dataset.num_features,\n",
    ")\n",
    "\n",
    "print(\"Data sample:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Applying the Lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_type = \"liftings\"\n",
    "transform_id = \"pointcloud2hypergraph/pointnet_lifting\"\n",
    "transform_config = {\"lifting\": load_transform_config(transform_type, transform_id)}\n",
    "\n",
    "# Adapt sampling ratio and cluster radius\n",
    "transform_config[\"lifting\"][\"sampling_ratio\"] = 0.2\n",
    "transform_config[\"lifting\"][\"cluster_radius\"] = 0.1\n",
    "\n",
    "lifted_data = PreProcessor(\n",
    "    data, transform_config, os.path.join(rootutils.find_root(), dataset_config.data_dir)\n",
    ")\n",
    "describe_data(lifted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hypergraph represents the first set abstraction layer that is used by [PointNet++](https://arxiv.org/abs/1706.02413). To construct a complet PointNet++ out of this, we would have to recursively apply the lifting while regarding the previous hyperedges as new \"hyper-nodes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models.hypergraph.unigcn import UniGCNModel\n",
    "\n",
    "model_type = \"hypergraph\"\n",
    "model_id = \"unigcn\"\n",
    "model_config = load_model_config(model_type, model_id)\n",
    "\n",
    "model = UniGCNModel(model_config, dataset_config)\n",
    "\n",
    "print(\"\\nModel output:\")\n",
    "model(lifted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
